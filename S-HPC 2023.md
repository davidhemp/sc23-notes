# 2nd International Workshop on Cyber Security in High Performance Computing (S-HPC 2023)
https://sc23.conference-program.com/session/?sess=sess429

## Introduction
Performance is everything? What about securing the inovation that HPC is driving and the data (privacy, confidentiality). Is this really about performance or is it just a hard problem where we need to find a balance. 

## Distinguished Speaker: The ‘S’ in HPC Stands for Security
https://sc23.conference-program.com/presentation/?id=misc267&sess=sess429

Ryan Adamson - Oak Ridge National Laboratory (ORNL)

HPC systems are designed to meet peak performance and scalability goals but today’s security guidance and tools are designed for enterprise infosec. This means that it is quite difficult to secure HPC resources without impacting performance goals. In this talk, we will examine the key security differences between enterprise systems and the common features of HPC environments. We will also discuss a new HPC Security NIST publication (currently draft) and touch on how secure ‘open science’ research really needs to be. From there, we will explore emerging trends to keep track of such as scientific workflows that span multiple security domains and whether trusted computing and zero trust models can be adapted to HPC. Finally, we will demonstrate one example of a zero-day vulnerability found on a previous #1 top 500 system (disclosed and patched in 2018) to help motivate broader action to put an ‘S’ in HPC.

<ins>Notes</ins>
HPC systems generally don't have emails, printers and other common attack vectors BUT users also have the ability to execute arbitrary code. We probabily can't use things like SELinux or other common security tools. 

The main goal of Security in HPC is to ensure that something a user is running is what the user wants to run and that we want them to run it. 

Common attack senarios
* Credential harvesting
* Remote exploitation
* Supply chain explotation (what libraries are you really using?)
* Accelerators, novel memory and storage technologies might not conserve security context

Other comments
* If we are the Sheriffs then scientists are the deputies as we need their help to secure the system and to check the software they are using if working correctly.
* Bleeding edge software is the most likely to have vulnerabilities.
* Is AI Correctness a security issue?
* Reproducibility - Using build chain tools like Spack, EB, containers. 
* IRI standardisation
* Scientific user workflows span security domains, they are the only ones that know the full scope so we have to talk to them and agree on methods.
* RDMA completely changes the security model as the Linux Kernel has no idea how to protect memory on other nodes.
* Context collapse, an Admin on one system could switch to a user that has access to another system they wouldn't normally have access to.
* Can we give the network more context of what is happening and use the network to single out issues. SciTokens, SciTags, Named Data networking. 

* NIST SP 800-53 helps identify controls
* NIST SP-223 HPC security architecture
* NIST SP 800-218 Secure software development framework

## Thoughts on Security for CXL-3.x-GFAM Clusters with Embedded Computing
https://sc23.conference-program.com/presentation/?id=misc269&sess=sess429

Craig Warner - Micron Technology Inc

Multi-host clusters built with CXL memory modules (enabled by the CXL 3.0 standard) provide a for an opportunity for power efficient computing. Enabling near data computing is not without it security challenges. This presentation identifies several security issues worthy of architectural considerations and approaches for mitigating these issues.

<ins>Notes</ins>
[CXL](https://www.computeexpresslink.org/) (Compute Express Link) is a memory fabric that can be used on x86. Requires CXL compatable switches. CXL3 allows a large memory device(s) to be connected to multi-nodes. It does this via a fabric that looks a lot like a tree network. The target is to have a latancy of ~600ns, slower than local memory but still very fast. Security here is clearly very important. What about using encryption? This is already a part of CXL v3 standard. AES-256-XTS looks like the simplest solution as it is already used on SSDs. This encryption only takes 20 ns do wouldn't impact the 600 ns target. The keys would be on the CXL devices. We could do host encryption but that doesn't allow near memory compute on the CXL module or compression. Although maybe that doesn't matter and the advantage of the data always being encrypted outside of the compute node is more important. For Host-side, all hosts would have to use the same key so sharing that would need to be done securely. 

## Information Security Controls Prioritization – SABSA for HPC
https://sc23.conference-program.com/presentation/?id=misc270&sess=sess429

Nicolas Erdody - Open Parallel Ltd. Multicore World

<ins>Notes</ins>

## Analyzing the Performance Impact of HPC Workloads with Gramine+SGX on 3rd Generation Xeon Scalable Processors
https://sc23.conference-program.com/presentation/?id=ws_shpc101&sess=sess429

Shinobu Miwa - University of Electro-Communications, Japan

Both 3rd generation Xeon scalable processors and Gramine 1.0, which potentially improves the performance of Intel SGX, were released in 2021. In this paper, we provide the first performance analysis of HPC workloads with Gramine and SGX on 3rd generation Xeon scalable processors. Our analysis starts with some microbenchmarks and is then extended to various HPC workloads. Our experimental results show that Gramine+SGX shows a small performance overhead (4-17%) for both compute-intensive and memory-bandwidth-sensitive workloads but a bit large performance overhead (up to 170%) for a memory-latency-sensitive workload. In addition, we show that the combination of Gramine and a 3rd generation Xeon scalable processor shows a slowdown of 1.5x on average (up to 4.4x) for many HPC workloads. This number is an order of magnitude smaller than that reported in the previous work using the combination of the former generation SGX toolchain and processor.

<ins>notes</ins>

## Panel: RMF for HPC and RDT&E

Moderator: Rickey Gregg - DoD
Panelists: Phillip Tartaglia, Ian Lee, James Waterman, Albert Reuther, Gary Keys

<ins>Notes</ins>
